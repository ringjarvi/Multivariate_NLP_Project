{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, HashingVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB\n",
    "from sklearn.metrics import classification_report, f1_score, accuracy_score\n",
    "from sklearn.svm import LinearSVC\n",
    "import numpy as np\n",
    "import spacy\n",
    "from numpy import savetxt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = fetch_20newsgroups(subset=\"all\", remove=('headers', 'footers', 'quotes'))\n",
    "#data = fetch_20newsgroups(subset=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['alt.atheism',\n",
       " 'comp.graphics',\n",
       " 'comp.os.ms-windows.misc',\n",
       " 'comp.sys.ibm.pc.hardware',\n",
       " 'comp.sys.mac.hardware',\n",
       " 'comp.windows.x',\n",
       " 'misc.forsale',\n",
       " 'rec.autos',\n",
       " 'rec.motorcycles',\n",
       " 'rec.sport.baseball',\n",
       " 'rec.sport.hockey',\n",
       " 'sci.crypt',\n",
       " 'sci.electronics',\n",
       " 'sci.med',\n",
       " 'sci.space',\n",
       " 'soc.religion.christian',\n",
       " 'talk.politics.guns',\n",
       " 'talk.politics.mideast',\n",
       " 'talk.politics.misc',\n",
       " 'talk.religion.misc']"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.target_names #categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18846,)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.filenames.shape #file names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DESCR', 'data', 'filenames', 'target', 'target_names']"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(data) #calls you can make on data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'From: tell@cs.unc.edu (Stephen Tell)\\nSubject: Re: subliminal message flashing on TV\\nOrganization: The University of North Carolina at Chapel Hill\\nLines: 25\\nNNTP-Posting-Host: rukbat.cs.unc.edu\\n\\nIn article <7480237@hpfcso.FC.HP.COM> myers@hpfcso.FC.HP.COM (Bob Myers) writes:\\n>> Hi.  I was doing research on subliminal suggestion for a psychology\\n>> paper, and I read that one researcher flashed hidden messages on the\\n>> TV screen at 1/200ths of a second.  Is that possible?\\n\\n> Might\\n>even be a vector (\"strokewriter\") display, in which case the lower limit\\n>on image time is anyone\\'s guess (and is probably phosphor-persistence limited).\\n\\nBack in high school I worked as a lab assistant for a bunch of experimental\\npsychologists at Bell Labs.  When they were doing visual perception and\\nmemory experiments, they used vector-type displays, with 1-millisecond\\nrefresh rates common.\\n\\nSo your case of 1/200th sec is quite practical, and the experimenters were\\nprobably sure that it was 5 milliseconds, not 4 or 6 either.\\n\\n>Bob Myers  KC0EW >myers@fc.hp.com \\n\\nSteve\\n-- \\nSteve Tell       tell@cs.unc.edu H: 919 968 1792  | #5L Estes Park apts\\nUNC Chapel Hill Computer Science W: 919 962 1845  | Carrboro NC 27510\\nEngineering is a _lot_ like art:  Some circuits are like lyric poems, some\\nare like army manuals, and some are like The Hitchhiker\\'s Guide to the Galaxy..\\n'"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.data[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.target[5] #groups each article belongs to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 14.3 s, sys: 722 ms, total: 15 s\n",
      "Wall time: 15.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "nlp = spacy.load(\"en_core_web_md\") #load spacy engligh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of stop words: 326\n",
      "First ten stop words: ['and', 'everything', \"'d\", 'regarding', 'thus', 'your', 'whence', 'which', 'fifty', 'other']\n"
     ]
    }
   ],
   "source": [
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "spacy_stopwords = spacy.lang.en.stop_words.STOP_WORDS\n",
    "print('Number of stop words: %d' % len(spacy_stopwords))\n",
    "print('First ten stop words: %s' % list(spacy_stopwords)[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "customize_stop_words = [\n",
    "    '\\n', '\\n\\n', '\\n\\n\\n', '\\n\\n\\n\\n'\n",
    "]\n",
    "for w in customize_stop_words:\n",
    "    nlp.vocab[w].is_stop = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#clean articles stop words, spaces, and puncuation removed\n",
    "for i in range(len(data.data)):\n",
    "    doc = nlp(data.data[i])\n",
    "\n",
    "    data.data[i] = [token.text for token in doc if not token.is_stop |token.is_punct |token.is_space]\n",
    "    # print('Original Article: %s' % (data.data[0]))\n",
    "    # print()\n",
    "    # print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sure',\n",
       " 'bashers',\n",
       " 'Pens',\n",
       " 'fans',\n",
       " 'pretty',\n",
       " 'confused',\n",
       " 'lack',\n",
       " 'kind',\n",
       " 'posts',\n",
       " 'recent',\n",
       " 'Pens',\n",
       " 'massacre',\n",
       " 'Devils',\n",
       " 'Actually',\n",
       " 'bit',\n",
       " 'puzzled',\n",
       " 'bit',\n",
       " 'relieved',\n",
       " 'going',\n",
       " 'end',\n",
       " 'non',\n",
       " 'PIttsburghers',\n",
       " 'relief',\n",
       " 'bit',\n",
       " 'praise',\n",
       " 'Pens',\n",
       " 'Man',\n",
       " 'killing',\n",
       " 'Devils',\n",
       " 'worse',\n",
       " 'thought',\n",
       " 'Jagr',\n",
       " 'showed',\n",
       " 'better',\n",
       " 'regular',\n",
       " 'season',\n",
       " 'stats',\n",
       " 'lot',\n",
       " 'fo',\n",
       " 'fun',\n",
       " 'watch',\n",
       " 'playoffs',\n",
       " 'Bowman',\n",
       " 'let',\n",
       " 'JAgr',\n",
       " 'lot',\n",
       " 'fun',\n",
       " 'couple',\n",
       " 'games',\n",
       " 'Pens',\n",
       " 'going',\n",
       " 'beat',\n",
       " 'pulp',\n",
       " 'Jersey',\n",
       " 'disappointed',\n",
       " 'Islanders',\n",
       " 'lose',\n",
       " 'final',\n",
       " 'regular',\n",
       " 'season',\n",
       " 'game',\n",
       " 'PENS',\n",
       " 'RULE']"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.data[0] #example of article after being cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "#changes articles back into string\n",
    "clean_articles=list()\n",
    "for i in range(len(data.data)):\n",
    "    temp=' '.join(word for word in data.data[i])\n",
    "    clean_articles.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'high school worked lab assistant bunch experimental psychologists Bell Labs visual perception memory experiments vector type displays 1-millisecond refresh rates common case 1/200th sec practical experimenters probably sure 5 milliseconds 4 6 Steve'"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_articles[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following are the 20 topics that an article can belong to:\n",
      "['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\n"
     ]
    }
   ],
   "source": [
    "text = clean_articles\n",
    "target = data[\"target\"]\n",
    "print(\"The following are the 20 topics that an article can belong to:\")\n",
    "print(data[\"target_names\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(text, target, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training dataset contains 14134 articles.\n",
      "The test dataset contains 4712 articles.\n"
     ]
    }
   ],
   "source": [
    "print(f\"The training dataset contains {len(X_train)} articles.\")\n",
    "print(f\"The test dataset contains {len(X_test)} articles.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vectorizes each article into a (300,1) vector\n",
    "X_train_glove= np.array([nlp(text).vector for text in X_train])\n",
    "X_test_glove= np.array([nlp(text).vector for text in X_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "#above is the spacy glove tokenization of the words. could try many others to see how the results come out. going to pass this data to R and see how it does with clusting agoritms\n",
    "\n",
    "savetxt('X_train.csv', X_train_glove, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "savetxt('X_test.csv', X_test_glove, delimiter=',')\n",
    "savetxt('y_test.csv', y_test, delimiter=',')\n",
    "savetxt('y_train.csv', y_train, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Uh slight clarification printer driver c.itoh LIPS10 laser printer Thanks'"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18846,)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.target.shape #target categories groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 17.8 s, sys: 483 ms, total: 18.3 s\n",
      "Wall time: 18.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Use English stopwords and produce a BoW representation for the data using up to trigrams\n",
    "# Save the vectorizer as counter and the transformed data as X_train_bow, and X_test_bow\n",
    "# YOUR CODE HERE\n",
    "counter = CountVectorizer(stop_words='english',ngram_range=(1, 3)).fit(X_train, y_train)\n",
    "X_train_bow = counter.transform(X_train)\n",
    "X_test_bow=counter.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4712, 2262337)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test=X_test_bow.toarray()\n",
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(test[0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.11 s, sys: 96.1 ms, total: 1.2 s\n",
      "Wall time: 1.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Use the BoW representation you just created above to produce a TFIDF representation of the data\n",
    "# Save the transformer to tfidfer and the transformed data as X_train_tfidf, and X_test_tfidf\n",
    "\n",
    "# YOUR CODE HERE\n",
    "tfidfer=TfidfTransformer()\n",
    "X_train_tfidf=tfidfer.fit_transform(X_train_bow)\n",
    "X_test_tfidf=tfidfer.transform(X_test_bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scipy.sparse.csr.csr_matrix"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
